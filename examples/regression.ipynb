{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc04e606",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "In this notebook we demonstrate how to fit a Gaussian process regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax import config\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jaxtyping import install_import_hook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from examples.utils import (\n",
    "    clean_legend,\n",
    "    use_mpl_style,\n",
    ")\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "    import gpjax as gpx\n",
    "\n",
    "\n",
    "key = jr.key(123)\n",
    "\n",
    "# set the default style for plotting\n",
    "use_mpl_style()\n",
    "\n",
    "cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aebc16",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "With the necessary modules imported, we simulate a dataset\n",
    "$\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}$ with inputs $\\boldsymbol{x}$\n",
    "sampled uniformly on $(-3., 3)$ and corresponding independent noisy outputs\n",
    "\n",
    "$$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4\\boldsymbol{x}) + \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.3^2 \\right).$$\n",
    "\n",
    "We store our data $\\mathcal{D}$ as a GPJax `Dataset` and create test inputs and labels\n",
    "for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16197a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "noise = 0.3\n",
    "\n",
    "key, subkey = jr.split(key)\n",
    "x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\n",
    "f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n",
    "signal = f(x)\n",
    "y = signal + jr.normal(subkey, shape=signal.shape) * noise\n",
    "\n",
    "D = gpx.Dataset(X=x, y=y)\n",
    "\n",
    "xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\n",
    "ytest = f(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad4f3b",
   "metadata": {},
   "source": [
    "To better understand what we have simulated, we plot both the underlying latent\n",
    "function and the observed data that is subject to Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30672e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\n",
    "ax.plot(xtest, ytest, label=\"Latent function\", color=cols[1])\n",
    "ax.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75394d12",
   "metadata": {},
   "source": [
    "Our aim in this tutorial will be to reconstruct the latent function from our noisy\n",
    "observations $\\mathcal{D}$ via Gaussian process regression. We begin by defining a\n",
    "Gaussian process prior in the next section.\n",
    "\n",
    "## Defining the prior\n",
    "\n",
    "A zero-mean Gaussian process (GP) places a prior distribution over real-valued\n",
    "functions $f(\\cdot)$ where\n",
    "$f(\\boldsymbol{x}) \\sim \\mathcal{N}(0, \\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}})$\n",
    "for any finite collection of inputs $\\boldsymbol{x}$.\n",
    "\n",
    "Here $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ is the Gram matrix generated by a\n",
    "user-specified symmetric, non-negative definite kernel function $k(\\cdot, \\cdot')$\n",
    "with $[\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}]_{i, j} = k(x_i, x_j)$.\n",
    "The choice of kernel function is critical as, among other things, it governs the\n",
    "smoothness of the outputs that our GP can generate.\n",
    "\n",
    "For simplicity, we consider a radial basis function (RBF) kernel:\n",
    "\n",
    "$$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\lVert x - x' \\rVert_2^2}{2 \\ell^2}\\right).$$\n",
    "\n",
    "On paper a GP is written as $f(\\cdot) \\sim \\mathcal{GP}(\\textbf{0}, k(\\cdot, \\cdot'))$,\n",
    "we can reciprocate this process in GPJax via defining a `Prior` with our chosen `RBF`\n",
    "kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0411e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gpx.kernels.RBF()  # 1-dimensional input\n",
    "meanf = gpx.mean_functions.Zero()\n",
    "prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f8da2",
   "metadata": {},
   "source": [
    "\n",
    "The above construction forms the foundation for GPJax's models. Moreover, the GP prior\n",
    "we have just defined can be represented by a\n",
    "[TensorFlow Probability](https://www.tensorflow.org/probability/api_docs/python/tfp/substrates/jax)\n",
    "multivariate Gaussian distribution. Such functionality enables trivial sampling, and\n",
    "the evaluation of the GP's mean and covariance.\n",
    "\n",
    "Since we want to sample from the full posterior, we need to calculate the full covariance matrix.\n",
    "We can enforce this by including the `return_covariance_type = \"dense\"` attribute when predicting.\n",
    "Note this is what will be defaulted if left blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ed9d50",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdf52c6d",
   "metadata": {},
   "source": [
    "prior_dist = prior.predict(xtest, return_covariance_type=\"dense\")\n",
    "\n",
    "prior_mean = prior_dist.mean\n",
    "prior_std = prior_dist.variance\n",
    "samples = prior_dist.sample(key=key, sample_shape=(20,))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\")\n",
    "ax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\")\n",
    "ax.fill_between(\n",
    "    xtest.flatten(),\n",
    "    prior_mean - prior_std,\n",
    "    prior_mean + prior_std,\n",
    "    alpha=0.3,\n",
    "    color=cols[1],\n",
    "    label=\"Prior variance\",\n",
    ")\n",
    "ax.legend(loc=\"best\")\n",
    "ax = clean_legend(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c6b1f",
   "metadata": {},
   "source": [
    "## Constructing the posterior\n",
    "\n",
    "Having defined our GP, we proceed to define a description of our data\n",
    "$\\mathcal{D}$ conditional on our knowledge of $f(\\cdot)$ --- this is exactly the\n",
    "notion of a likelihood function $p(\\mathcal{D} | f(\\cdot))$. While the choice of\n",
    "likelihood is a critical in Bayesian modelling, for simplicity we consider a\n",
    "Gaussian with noise parameter $\\alpha$\n",
    "\n",
    "$$p(\\mathcal{D} | f(\\cdot)) = \\mathcal{N}(\\boldsymbol{y}; f(\\boldsymbol{x}), \\textbf{I} \\alpha^2).$$\n",
    "\n",
    "This is defined in GPJax through calling a `Gaussian` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e858782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631897a9",
   "metadata": {},
   "source": [
    "The posterior is proportional to the prior multiplied by the likelihood, written as\n",
    "\n",
    "  $$ p(f(\\cdot) | \\mathcal{D}) \\propto p(f(\\cdot)) * p(\\mathcal{D} | f(\\cdot)). $$\n",
    "\n",
    "Mimicking this construct, the posterior is established in GPJax through the `*` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd20139",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = prior * likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c8820",
   "metadata": {},
   "source": [
    "<!-- ## Hyperparameter optimisation\n",
    "\n",
    "Our kernel is parameterised by a length-scale $\\ell^2$ and variance parameter\n",
    "$\\sigma^2$, while our likelihood controls the observation noise with $\\alpha^2$.\n",
    "Using Jax's automatic differentiation module, we can take derivatives of  -->\n",
    "\n",
    "## Parameter state\n",
    "\n",
    "As outlined in the [PyTrees](https://jax.readthedocs.io/en/latest/pytrees.html)\n",
    "documentation, parameters are contained within the model and for the leaves of the\n",
    "PyTree. Consequently, in this particular model, we have three parameters: the\n",
    "kernel lengthscale, kernel variance and the observation noise variance. Whilst\n",
    "we have initialised each of these to 1, we can learn Type 2 MLEs for each of\n",
    "these parameters by optimising the marginal log-likelihood (MLL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-gpx.objectives.conjugate_mll(posterior, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305b9de",
   "metadata": {},
   "source": [
    "We can now define an optimiser. For this example we'll use the `bfgs`\n",
    "optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_posterior, history = gpx.fit_scipy(\n",
    "    model=posterior,\n",
    "    objective=lambda p, d: -gpx.objectives.conjugate_mll(p, d),\n",
    "    train_data=D,\n",
    "    trainable=gpx.parameters.Parameter,\n",
    ")\n",
    "\n",
    "print(-gpx.objectives.conjugate_mll(opt_posterior, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca23fde3",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Equipped with the posterior and a set of optimised hyperparameter values, we are now\n",
    "in a position to query our GP's predictive distribution at novel test inputs. To do\n",
    "this, we use our defined `posterior` and `likelihood` at our test inputs to obtain\n",
    "the predictive distribution as a `Distrax` multivariate Gaussian upon which `mean`\n",
    "and `stddev` can be used to extract the predictive mean and standard deviatation.\n",
    "\n",
    "We are only concerned here about the variance between the test points and themselves, so\n",
    "we can just copute the diagonal version of the covariance.  We enforce this by using\n",
    "`return_covariance_type = \"diagonal\"` in the `predict` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971bbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dist = opt_posterior.predict(xtest, train_data=D, return_covariance_type=\"diagonal\")\n",
    "predictive_dist = opt_posterior.likelihood(latent_dist)\n",
    "\n",
    "predictive_mean = predictive_dist.mean\n",
    "predictive_std = jnp.sqrt(predictive_dist.variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34135c",
   "metadata": {},
   "source": [
    "With the predictions and their uncertainty acquired, we illustrate the GP's\n",
    "performance at explaining the data $\\mathcal{D}$ and recovering the underlying\n",
    "latent function of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554bcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7.5, 2.5))\n",
    "ax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\n",
    "ax.fill_between(\n",
    "    xtest.squeeze(),\n",
    "    predictive_mean - 2 * predictive_std,\n",
    "    predictive_mean + 2 * predictive_std,\n",
    "    alpha=0.2,\n",
    "    label=\"Two sigma\",\n",
    "    color=cols[1],\n",
    ")\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    predictive_mean - 2 * predictive_std,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1,\n",
    "    color=cols[1],\n",
    ")\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    predictive_mean + 2 * predictive_std,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1,\n",
    "    color=cols[1],\n",
    ")\n",
    "ax.plot(\n",
    "    xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n",
    ")\n",
    "ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80550d2",
   "metadata": {},
   "source": [
    "## System configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9fd387",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -n -u -v -iv -w -a 'Thomas Pinder & Daniel Dodd'"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
